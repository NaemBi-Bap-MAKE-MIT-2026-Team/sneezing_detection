# ğŸ¤§ Sneeze Detection System

A comprehensive machine learning project for real-time sneeze detection using MFCC features and lightweight CNN models, optimized for embedded systems like Raspberry Pi and Jetson Nano.

## ğŸ¯ Project Overview

This project consists of two main components:

1. **Model Training** ([`sneeze_detection_lightweight.ipynb`](sneeze_detection_lightweight.ipynb)): Training pipeline for a lightweight CNN model using MFCC features
2. **Real-time Detection** ([`realtime_detection/`](realtime_detection/)): Production-ready real-time inference system

### Key Features

- âœ… **Lightweight Architecture**: ~50K parameters for efficient inference on embedded devices
- âœ… **MFCC-based Features**: Industry-standard audio feature extraction with Delta and Delta-Delta
- âœ… **Data Augmentation**: Time stretching, pitch shifting, noise addition for robust training
- âœ… **Improved Negative Sampling**: Focused on life noise (human-related sounds) for better discrimination
- âœ… **Modular Design**: Clean separation of concerns for maintainability and testing
- âœ… **Real-time Processing**: ~2 seconds latency with continuous audio monitoring
- âœ… **Raspberry Pi Ready**: Optimized for embedded systems deployment

## ğŸ“ Project Structure

```
sneeze_detection/
â”œâ”€â”€ README.md
â”œâ”€â”€ src/                                        # RPi deployment (v4, modular)
â”‚   â”œâ”€â”€ main.py                                 # Entry point (local / network mode)
â”‚   â”œâ”€â”€ v4_model.tflite                         # TFLite model weights
â”‚   â”œâ”€â”€ v4_norm_stats.npz                       # Z-score normalisation statistics
â”‚   â”œâ”€â”€ communication/                          # Unified audio-input layer
â”‚   â”‚   â”œâ”€â”€ send.py                             # AudioSender  â€” mic â†’ UDP stream
â”‚   â”‚   â”œâ”€â”€ recv.py                             # NetworkMicStream â€” UDP â†’ mic API
â”‚   â”‚   â”œâ”€â”€ README.md                           # Communication module docs
â”‚   â”‚   â”œâ”€â”€ example_local.py                    # Local mic example
â”‚   â”‚   â””â”€â”€ example_network.py                  # Network sender/receiver example
â”‚   â”œâ”€â”€ microphone/
â”‚   â”‚   â””â”€â”€ mic_input.py                        # MicrophoneStream (sounddevice)
â”‚   â”œâ”€â”€ ml_model/
â”‚   â”‚   â”œâ”€â”€ config.py                           # All constants (SR, thresholds, â€¦)
â”‚   â”‚   â”œâ”€â”€ preprocessing.py                    # rms, logmel, preproc, load_stats
â”‚   â”‚   â””â”€â”€ model.py                            # LiteModel (TFLite wrapper)
â”‚   â”œâ”€â”€ output_feature/
â”‚   â”‚   â”œâ”€â”€ speaker_output.py                   # SpeakerOutput (alert / beep)
â”‚   â”‚   â””â”€â”€ lcd_output.py                       # LCD + LCDAnimator (ST7789)
â”‚   â””â”€â”€ tests/
â”‚       â”œâ”€â”€ test_ml_model.py                    # Preprocessing & inference tests
â”‚       â””â”€â”€ test_communication.py               # UDP loopback tests
â”œâ”€â”€ legacy_code/                                # v1â€“v4 reference implementations
â”‚   â””â”€â”€ output/v4/
â”‚       â”œâ”€â”€ test_no_saving_v4.py
â”‚       â””â”€â”€ test_saving_v4.py
â”œâ”€â”€ raspi/sneeze-detection/
â”‚   â””â”€â”€ real_time_detection.py                  # Original RPi script (reference)
â””â”€â”€ sneeze_detection_lightweight.ipynb          # Model training notebook
```

## ğŸš€ Quick Start

### 1. Training the Model

Open and run the Jupyter notebook:

```bash
jupyter notebook sneeze_detection_lightweight.ipynb
```

### 2. Real-time Detection (`src/`)

Place the model files in `src/` before running:

```
src/
â”œâ”€â”€ v4_model.tflite
â””â”€â”€ v4_norm_stats.npz
```

Also create the asset directory (images + sound for LCD and speaker):

```
~/Documents/sneeze-detection/
    images/  idle.png  detect1.png  detect2.png  detect3.png
    sounds/  bless_you.wav
```

---

#### Mode A â€” Local microphone (single device)

The simplest setup: microphone and inference run on the same Raspberry Pi.

```bash
cd src

# With LCD
python main.py

# Without LCD (no ST7789 hardware attached)
python main.py --no-lcd
```

---

#### Mode B â€” Network streaming (two devices)

Use this when the **microphone is on one device** (e.g. a PC or second RPi) and
**inference runs on another** (e.g. the main Raspberry Pi).

```
[Device A â€” Sender]                [Device B â€” Receiver / RPi]
  has microphone                     runs inference + LCD + speaker

  python send.py                     python main.py --network
    --host <Device-B-IP>               --recv-host 0.0.0.0
    --port 12345                       --recv-port 12345
                                       --no-lcd          â† optional
```

**Device A â€” Sender** (machine with microphone):

```bash
cd src
python communication/send.py --host 192.168.1.42 --port 12345
```

**Device B â€” Receiver** (Raspberry Pi running inference):

```bash
cd src
python main.py --network --recv-host 0.0.0.0 --recv-port 12345
```

Loopback test on a single machine (both terminals in `src/`):

```bash
# Terminal 1
python main.py --network --no-lcd

# Terminal 2
python communication/send.py --host 127.0.0.1 --port 12345
```

---

#### `main.py` â€” All arguments

| Argument | Default | Description |
| --- | --- | --- |
| *(none)* | â€” | Local microphone, LCD enabled |
| `--network` | off | Receive audio from `send.py` over UDP |
| `--recv-host` | `0.0.0.0` | `[--network]` UDP bind address |
| `--recv-port` | `12345` | `[--network]` UDP port |
| `--no-lcd` | off | Disable the ST7789 LCD driver |

#### `send.py` â€” All arguments

| Argument | Default | Description |
| --- | --- | --- |
| `--host` | `127.0.0.1` | Destination IP (receiver device) |
| `--port` | `12345` | Destination UDP port |
| `--capture-sr` | `48000` | Microphone sample rate (Hz) |
| `--block-ms` | `10` | Packet size in ms (lower = less latency) |
| `--device` | system default | sounddevice input device index or name |

---

### 3. Tests

```bash
cd src
python tests/test_ml_model.py        # preprocessing + TFLite inference
python tests/test_communication.py   # UDP loopback (no mic required)
```

## ğŸ“Š Model Performance

### Training Results
- **Architecture**: LightweightSneezeCNN (Depthwise Separable Convolutions)
- **Parameters**: ~20,751 (~50K total with batch norm)
- **Model Size**: ~0.2 MB (FP32)
- **Test Accuracy**: >90% (on validation set)
- **F1 Score**: ~0.87-0.90

### Real-time Performance
- **Processing Time**: 80-120ms per 2-second window
  - Preprocessing: ~10ms
  - MFCC Extraction: ~60-80ms
  - Model Inference: ~15-25ms
- **Latency**: ~2.1 seconds (window + processing)
- **CPU Usage**: 25-40% (Raspberry Pi 4)
- **Memory**: ~60MB

## ğŸ§  Technical Details

### Data Pipeline

```
Audio (16kHz, 2s) â†’ Preprocessing â†’ MFCC (60Ã—63) â†’ Model â†’ Probability
```

#### Preprocessing Steps
1. **RMS Normalization**: Target RMS = 0.1
2. **Pre-emphasis**: Î± = 0.97 (high-frequency boost)
3. **Silence Trimming**: top_db = 20

#### MFCC Features
- **Base Coefficients**: 20 MFCC
- **Delta Features**: 20 (first-order derivative)
- **Delta-Delta Features**: 20 (second-order derivative)
- **Total**: 60 features Ã— 63 time frames

### Model Architecture

```python
LightweightSneezeCNN(
    Input: (1, 1, 60, 63)
    â†’ DepthwiseSeparableConv2D (1â†’32)
    â†’ MaxPool2D
    â†’ DepthwiseSeparableConv2D (32â†’64)
    â†’ MaxPool2D
    â†’ DepthwiseSeparableConv2D (64â†’128)
    â†’ MaxPool2D
    â†’ GlobalAveragePooling
    â†’ FC(128â†’64) + Dropout(0.3)
    â†’ FC(64â†’2)
    Output: (1, 2)
)
```

### Data Augmentation

The training pipeline includes:
- **Time Stretching**: 0.8x - 1.2x speed variation
- **Pitch Shifting**: Â±3 semitones
- **Noise Addition**: Gaussian noise (0.002-0.01 factor)
- **Time Shifting**: Â±20% temporal shift

### Negative Sample Selection

Instead of random environmental sounds, the model uses **life noise** (human-related sounds) for better discrimination:
- Coughing, breathing, laughing
- Footsteps, door knocks, clapping
- Keyboard typing, drinking, brushing teeth

This approach significantly improves classification accuracy by creating clearer class boundaries.

## ğŸ“¦ Dependencies

### Core Libraries
- `torch>=2.0.0` - Deep learning framework
- `librosa>=0.10.0` - Audio feature extraction
- `pyaudio>=0.2.13` - Real-time audio capture
- `numpy>=1.24.0` - Numerical computing
- `soundfile>=0.12.1` - Audio file I/O

### For Training
- `polars` - Fast dataframe operations
- `matplotlib` - Visualization
- `scikit-learn` - ML utilities

See [`realtime_detection/requirements.txt`](realtime_detection/requirements.txt) for complete list.

## ğŸ“ Dataset

### Training Data
- **Positive Samples**: 968 sneeze recordings
- **Negative Samples**: 968 life noise samples from ESC-50
  - Categories: coughing, breathing, laughing, footsteps, etc.
- **Augmentation**: 2x data (1936 positive + 1936 negative)

### ESC-50 Dataset
The project uses [ESC-50](https://github.com/karolpiczak/ESC-50) (Environmental Sound Classification) dataset for negative samples.

```bash
# Download ESC-50 (if needed)
git clone https://github.com/karolpiczak/ESC-50.git esc-50
```

## ğŸ”§ Configuration

Key parameters can be adjusted in [`realtime_detection/utils/config.py`](realtime_detection/utils/config.py):

```python
# Audio Parameters
SAMPLE_RATE = 16000      # Audio sample rate (Hz)
WINDOW_SIZE = 32000      # Analysis window (2 seconds)
THRESHOLD = 0.8          # Detection threshold

# MFCC Parameters (DO NOT CHANGE - must match training)
N_MFCC = 20              # MFCC coefficients
N_FFT = 2048             # FFT window size
HOP_LENGTH = 512         # Frame stride
INCLUDE_DELTAS = True    # Delta + Delta-Delta features
```

## ğŸ§ª Testing

Test individual components:

```bash
# Test model inference
cd realtime_detection
python -m modules.model_inference

# Test MFCC extraction
python -m modules.mfcc_extractor

# Test audio capture
python -m modules.audio_capture
```

## ğŸ“ˆ Future Improvements

### Short-term
- [ ] Export to ONNX for faster inference
- [ ] INT8 quantization for Raspberry Pi
- [ ] Multi-threading for parallel processing
- [ ] Web interface for monitoring

### Long-term
- [ ] Edge TPU support (Google Coral)
- [ ] RNN/LSTM for lower latency
- [ ] Multi-class detection (sneeze, cough, speech)
- [ ] Mobile app with on-device inference
- [ ] Cloud integration for logging

## ğŸ¤– Built with AI

This project was developed collaboratively with **[Claude](https://claude.ai)** (Anthropic's AI assistant) using the **[Claude Code](https://claude.ai/claude-code)** CLI tool.

### Development Process
1. **Planning**: Analyzed training pipeline and designed modular architecture
2. **Implementation**: Systematically built each module with proper separation of concerns
3. **Testing**: Created comprehensive test cases for each component
4. **Documentation**: Generated detailed documentation and usage guides

The entire system - from initial exploration to production-ready deployment - was created through interactive AI-assisted development, demonstrating the power of human-AI collaboration in software engineering.

## ğŸ‘¨â€ğŸ’» Author

**Bahk Insung**
- GitHub: [@bahk_insung](https://github.com/bahk_insung)
- Developed in collaboration with Claude (Anthropic)
- Date: February 2026

## ğŸ“„ License

This project is for research and educational purposes.

## ğŸ™ Acknowledgments

- **Training Data**: [ESC-50](https://github.com/karolpiczak/ESC-50) environmental sound dataset
- **Model Architecture**: Inspired by MobileNet's Depthwise Separable Convolutions
- **MFCC Implementation**: [Librosa](https://librosa.org/) library
- **AI Development**: [Claude](https://claude.ai) by Anthropic
- **Development Tool**: [Claude Code](https://claude.ai/claude-code) CLI

## ğŸ“š References

- [Librosa Documentation](https://librosa.org/doc/latest/index.html)
- [PyTorch Documentation](https://pytorch.org/docs/stable/index.html)
- [MFCC Tutorial](https://en.wikipedia.org/wiki/Mel-frequency_cepstrum)
- [Depthwise Separable Convolutions](https://paperswithcode.com/method/depthwise-separable-convolution)
- [ESC-50 Dataset Paper](http://karol.piczak.com/papers/Piczak2015-ESC-Dataset.pdf)

## ğŸ“§ Contact

For questions, issues, or contributions, please open an issue on GitHub.

---

**ğŸ¯ Built with Human-AI Collaboration Â· ğŸ¤§ Powered by Machine Learning Â· ğŸš€ Ready for Production**
